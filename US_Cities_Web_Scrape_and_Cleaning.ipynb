{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Trends in Most Populous Cities of the United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Company X wishes to branch out their business chain by opening new locations in ideal US cities. To identify which US cities might be ideal candidates we explore relationships between the most populous US cities and the types of venues that correspond to each. We will then perform agglomerative hierarchical clustering to see which of the top 200 most populous US cities might be worthy of looking into further. The hierarchical clustering will be based on population, population change (percent increase/decrease), and most common types of venues that exist within them. Once the hierarchical clustering is completed, a more in-depth study can then be performed by Company X. Therefore, this work is a preliminary investigation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "To start with, we scrape Wikipedia's page for the list of most populous US cities with their corresponding percent-increase/percent-decrease:\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
    "\n",
    "Then, using this initial data set with the corresponding GPS locations (included in the data table), we will obtain venues using the Foursquare API and investigate the trending venues.\n",
    "\n",
    "I perform the scrape below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import web scraping tools\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# library to handle JSON files\n",
    "import json\n",
    "\n",
    "# import geocoder\n",
    "import geocoder\n",
    "\n",
    "# convert an address into latitude and longitude values\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# import folium for maps\n",
    "import folium\n",
    "\n",
    "# library to handle requests\n",
    "import requests\n",
    "\n",
    "# tranform JSON file into a pandas dataframe\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_contents(page_url):\n",
    "    results = requests.get(page_url) # access url location\n",
    "    soup = BeautifulSoup(results.text,'html.parser') # parse through html of url and store page info\n",
    "    return soup\n",
    "    \n",
    "cities_url = 'https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population' # define desired url\n",
    "\n",
    "cities_contents = get_html_contents(cities_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_pop_table = cities_contents.find('table',{'class': 'wikitable sortable'}) # find the first table of class 'wikitable sortable'\n",
    "city_pop_header = city_pop_table.tr.find_all('th') # create html table headers list\n",
    "\n",
    "column_names = [] # create empty list\n",
    "\n",
    "for header in city_pop_header:\n",
    "    column_names.append(header.text.strip().replace(',','_')) # collects column names as list\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'us_cities_data_2020.csv' # give name to file\n",
    "\n",
    "f = open(file_name,'w+', encoding='utf-8') # utf-8 encoding is used in html;\n",
    "# will need to clean up\n",
    "\n",
    "for i in range(len(column_names)): # write to file all column names as header to file\n",
    "    if column_names[i] != column_names[-1]:\n",
    "        f.write(column_names[i] + ',')\n",
    "    else:\n",
    "        f.write(column_names[i] + '\\n')\n",
    "\n",
    "table_rows = city_pop_table.find_all('tr') # list of all tr\n",
    "\n",
    "for row in table_rows: # grab data from each cell and write to file\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "        for i, cell in enumerate(cells):\n",
    "            cell_data = cell.text.strip().replace(',','')\n",
    "            if cells[i] != cells[-1]:\n",
    "                if i == 7 or i == 9: # ignore metric units\n",
    "                # assuming this is an American company so English units are desired\n",
    "                    continue\n",
    "                f.write(cell_data + ',')\n",
    "            else:\n",
    "                f.write(cell_data + '\\n')\n",
    "                    \n",
    "f.close() # be sure to close the file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df = pd.read_csv('us_cities_data_2020.csv') # read csv file to obtain dataframe\n",
    "us_cities_df.head() # take a peak at dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_cleaned = [] # initialize an empty list\n",
    "\n",
    "for i, text in enumerate(us_cities_df.index): # cleans city names and appends to list\n",
    "    cities_cleaned.append(us_cities_df.loc[i,'City'].split('[')[0])\n",
    "    \n",
    "print(cities_cleaned) # take a peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df['City'] = pd.Series(cities_cleaned) # repleace 'City' column with newly cleaned names\n",
    "us_cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_cleaned = [] # initialize empty list\n",
    "\n",
    "for i, content in enumerate(us_cities_df.index): # cleaning location lat/long coords\n",
    "    location_cleaned.append(us_cities_df.loc[i,'Location'].replace('\\ufeff','').split('/')[2].split('(')[0].replace(' ',''))\n",
    "    \n",
    "lat_coords = [] # initialize\n",
    "long_coords = [] # initialize\n",
    "\n",
    "for i, text in enumerate(location_cleaned): # separate lat and long coords into separate lists\n",
    "    lat, long = location_cleaned[i].split(';')\n",
    "    lat_coords.append(lat)\n",
    "    long_coords.append(long)\n",
    "\n",
    "us_cities_df['Lat'] = pd.Series(lat_coords).astype(float) # add column for latidude\n",
    "us_cities_df['Long'] = pd.Series(long_coords).astype(float) # add column for longitude\n",
    "us_cities_df.head() # take a peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df.drop('Location',axis=1,inplace=True) # drop messy location column\n",
    "us_cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df.rename(columns={'2020rank': '2020 Rank',\n",
    "                             'State[c]': 'State',\n",
    "                             '2020estimate': '2018 Estimate',\n",
    "                             '2010census': '2010 Census',\n",
    "                             '2016 land area': 'Land Area (sq mi)',\n",
    "                             '2016 population density': 'Population Density (per sq mi)'},\n",
    "                    inplace=True) # renaming columns for cleaner look and easier use\n",
    "us_cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df.drop('2020 Rank',axis=1,inplace=True) # rank unnecessary\n",
    "us_cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_area = [] # initialize\n",
    "pop_density = [] # initialize\n",
    "\n",
    "for i, text in enumerate(us_cities_df.index): # clean land area and population density columns\n",
    "    land_area.append(us_cities_df.loc[i,'Land Area (sq mi)'].replace('\\xa0sq\\xa0mi',''))\n",
    "    pop_density.append(us_cities_df.loc[i,'Population Density (per sq mi)'].split('/')[0])\n",
    "\n",
    "us_cities_df['Land Area (sq mi)'] = pd.Series(land_area).astype(float)\n",
    "us_cities_df['Population Density (per sq mi)'] = pd.Series(pop_density).astype(float)\n",
    "us_cities_df.head() # take a peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_change_float = []\n",
    "\n",
    "for i, content in enumerate(us_cities_df.index):\n",
    "    per_change_float.append(us_cities_df.loc[i,'Change'].replace('+','').replace('%','').replace('âˆ’','-'))\n",
    "\n",
    "us_cities_df['Change'] = pd.Series(per_change_float)\n",
    "\n",
    "for i, content in enumerate(us_cities_df['Change']):\n",
    "    try:\n",
    "        us_cities_df.loc[i,'Change'] = float(us_cities_df.loc[i,'Change']) # attempt to convert string to float\n",
    "    except:\n",
    "        us_cities_df.drop(index=i,inplace=True) # throw exception for new cities and/or unkown % change\n",
    "    \n",
    "us_cities_df.rename(columns={'Change': '% Change (Pop)'},inplace=True)\n",
    "us_cities_df['% Change (Pop)'] = us_cities_df['% Change (Pop)'].astype(float)\n",
    "us_cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df.info() # look at data types within dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_influx_df = us_cities_df[us_cities_df['% Change (Pop)'] >= 20.0].reset_index(drop=True) # define high\n",
    "# influx of population dataframe\n",
    "high_influx_df.head(high_influx_df.shape[0]) # take a peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_influx_df.to_csv('High_Influx_US_Cities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the lat/long coords for United States\n",
    "\n",
    "high_influx_df = pd.read_csv('High_Influx_US_Cities.csv')\n",
    "\n",
    "address = 'United States'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"us_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinates of the United States are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of United States using latitude and longitude values\n",
    "map_us_cities = folium.Map(location=[latitude, longitude], zoom_start=4)\n",
    "\n",
    "# add markers to map; color coded by % change (20%+)\n",
    "for lat, lng, city, state, inflx in zip(high_influx_df['Lat'],\n",
    "                                        high_influx_df['Long'],\n",
    "                                        high_influx_df['City'],\n",
    "                                        high_influx_df['State'],\n",
    "                                        high_influx_df['% Change (Pop)']):\n",
    "    \n",
    "    label = '{}, {}, {}\\% increase'.format(city, state, inflx)\n",
    "    \n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    \n",
    "    # separate % ranges by different colors\n",
    "    if inflx < 30: \n",
    "        marker_color='blue'\n",
    "    elif inflx < 40:\n",
    "        marker_color='orange'\n",
    "    elif inflx < 50:\n",
    "        marker_color='green'\n",
    "    else:\n",
    "        marker_color='red'\n",
    "        \n",
    "    # adding markers\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=marker_color,\n",
    "        fill=True,\n",
    "        fill_color=marker_color,\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_us_cities)  \n",
    "    \n",
    "map_us_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leverage Foursquare API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "CLIENT_ID = 'RNIRYZLUNKHD0IONCWKPVPRB4AZFALKN3ZECYPRETDCQYHJF' # your Foursquare ID\n",
    "CLIENT_SECRET = 'NLKUJQCLB05PWECWTK3VN5VV0PHNGGQEO2CVTCIJJQ2NGARL' # your Foursquare Secret\n",
    "ACCESS_TOKEN = 'MGILLFTINOBH02B4H2GBOROCUY2IUWUV5OZ2NFT2FTQANMFX' # your FourSquare Access Token\n",
    "VERSION = '20180604'\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_influx_df = pd.read_csv('High_Influx_US_Cities.csv')\n",
    "\n",
    "city = high_influx_df.City\n",
    "state = high_influx_df.State\n",
    "latitude = high_influx_df.Lat\n",
    "longitude = high_influx_df.Long\n",
    "cities_tuples = list(zip(city + ', ' + state, latitude, longitude))\n",
    "print(cities_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start looping through city centers and grabbing max num of venues away from city centers\n",
    "\n",
    "LIMIT = 100\n",
    "radius = 16100\n",
    "\n",
    "cities_dfs = {}\n",
    "cities_maps = {}\n",
    "\n",
    "for i in range(len(cities_tuples)):\n",
    "    \n",
    "    # select city with lat/long coords\n",
    "    city = cities_tuples[i][0]\n",
    "    latitude = cities_tuples[i][1]\n",
    "    longitude = cities_tuples[i][2]\n",
    "    \n",
    "    # define url to get request from\n",
    "    url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&ll={},{}&v={}&radius={}&limit={}'.format(CLIENT_ID, \n",
    "                                                                                                                     CLIENT_SECRET, \n",
    "                                                                                                                     latitude, \n",
    "                                                                                                                     longitude, \n",
    "                                                                                                                     VERSION, \n",
    "                                                                                                                     radius, \n",
    "                                                                                                                     LIMIT)\n",
    "    # print the current url being fetched\n",
    "    print(url)\n",
    "    # send get request and examine results for each city\n",
    "\n",
    "    # get the request from the current url and print info found\n",
    "    results = requests.get(url).json()\n",
    "    print(\n",
    "        '\\n\\nFound at least {} venues within {} km (~10 mi) from {}\\'s city center.\\n'.format(len(results['response']['groups'][0]['items']), \n",
    "                                                                                       radius / 1000, \n",
    "                                                                                       city))\n",
    "    \n",
    "    # define venue items\n",
    "    items = results['response']['groups'][0]['items']\n",
    "    \n",
    "    # flatten JSON\n",
    "    dataframe = pd.json_normalize(items)\n",
    "\n",
    "    # filter columns\n",
    "    filtered_columns = ['venue.name', 'venue.categories'] + [col for col in dataframe.columns if col.startswith('venue.location.')] + ['venue.id']\n",
    "    dataframe_filtered = dataframe.loc[:, filtered_columns]\n",
    "\n",
    "    # filter the category for each row\n",
    "    dataframe_filtered['venue.categories'] = dataframe_filtered.apply(get_category_type, axis=1)\n",
    "\n",
    "    # clean columns\n",
    "    dataframe_filtered.columns = [col.split('.')[-1] for col in dataframe_filtered.columns]\n",
    "\n",
    "    # add current city's venue dataframe to dataframe list\n",
    "    cities_dfs.update({city : dataframe_filtered})\n",
    "    \n",
    "    # define current city's map\n",
    "    venues_map = folium.Map(location=[latitude, longitude], zoom_start=12) # generate map centred around Ecco\n",
    "\n",
    "\n",
    "    # add current city center as a red circle mark\n",
    "    folium.CircleMarker(\n",
    "        [latitude, longitude],\n",
    "        radius=10,\n",
    "        popup=city,\n",
    "        fill=True,\n",
    "        color='red',\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.6\n",
    "        ).add_to(venues_map)\n",
    "\n",
    "\n",
    "    # add current city's popular spots to the current map as blue circle markers\n",
    "    for lat, lng, label in zip(dataframe_filtered.lat, dataframe_filtered.lng, 'Name: ' + dataframe_filtered.name + '\\n' + 'Category: ' + dataframe_filtered.categories):\n",
    "        folium.CircleMarker(\n",
    "            [lat, lng],\n",
    "            radius=5,\n",
    "            popup=label,\n",
    "            min_width=500,\n",
    "            max_width=500,\n",
    "            fill=True,\n",
    "            color='blue',\n",
    "            fill_color='blue',\n",
    "            fill_opacity=0.6,\n",
    "            parse_html=False\n",
    "            ).add_to(venues_map)\n",
    "    \n",
    "    # add current city's map to the dict of city maps\n",
    "    cities_maps.update({city : venues_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cities_maps keys (keys are the same for cities_dfs)\n",
    "cities_maps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peak at Austin, Texas venues\n",
    "cities_maps['Austin, Texas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peak at Austin, Texas df\n",
    "cities_dfs['Austin, Texas'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating venue counts dataframe to be used as features with cities as indeces\n",
    "\n",
    "features_df = pd.DataFrame()\n",
    "\n",
    "for key in cities_dfs.keys():\n",
    "    features_df.loc[len(features_df), 'City'] = key\n",
    "\n",
    "features_df = features_df.set_index('City')\n",
    "\n",
    "for key, value in cities_dfs.items():\n",
    "    col_list = sorted(list(value['categories'].unique()))\n",
    "    counts_list = list(value['categories'].value_counts().sort_index(ascending=True))\n",
    "    for i in range(len(col_list)):\n",
    "        features_df.loc[key, col_list[i]] = counts_list[i]\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore total number of venue types amongst all 48 cities\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all NaN values with 0\n",
    "features_df.replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peak\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update features df so that population and population increase are accounted for\n",
    "features_df['2018 Estimate'] = high_influx_df['2018 Estimate'].values\n",
    "features_df['% Change (Pop)'] = high_influx_df['% Change (Pop)'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peak\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for normalizing numerical data\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data_scaled_w_pop = normalize(features_df)\n",
    "data_scaled_w_pop = pd.DataFrame(data_scaled_w_pop, columns=features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peak\n",
    "data_scaled_w_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the \"2018 Estimate\" (population estimate in 2018) is not necessarily all the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_w_pop['2018 Estimate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering among most populous US cities with highest % increase included in features\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "def llf(id):\n",
    "    return '%s' % (features_df.index[id])\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.title('Most Populous US Cities Based on Top-Rated Venue Type and Pop. Increase', fontsize=20)\n",
    "dend = shc.dendrogram(shc.linkage(data_scaled_w_pop, method='ward'), leaf_label_func=llf, leaf_font_size=12)\n",
    "plt.axhline(y=0.0006, color='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing colored boxes around the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (15, 10))\n",
    "plt.title('Most Populous US Cities Based on Top-Rated Venue Type and Pop. Increase', fontsize=20)\n",
    "dend = shc.dendrogram(shc.linkage(data_scaled_w_pop, method='ward'), leaf_label_func=llf, leaf_font_size=12)\n",
    "plt.axhline(y=0.0006, color='k', linestyle='--')\n",
    "for coll in ax.collections[:-1]:  # the last collection is the ungrouped level\n",
    "    xmin, xmax = np.inf, -np.inf\n",
    "    ymax = -np.inf\n",
    "    for p in coll.get_paths():\n",
    "        box = p.get_extents()\n",
    "        (x0, _), (x1, y1) = p.get_extents().get_points()\n",
    "        xmin = min(xmin, x0)\n",
    "        xmax = max(xmax, x1)\n",
    "        ymax = max(ymax, y1)\n",
    "    rec = plt.Rectangle((xmin - 4, 0), xmax - xmin + 8, ymax*1.05,\n",
    "                        facecolor=coll.get_color()[0], alpha=0.2, edgecolor=\"none\")\n",
    "    ax.add_patch(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right, red-colored cluster contains a sub-cluster just to the left of the lower-right corner of the dendrogram. This sub-cluster consists of five cities: Denver, CO; Seattle, WA; Charlotte, NC; Austin, TX; and Fort Worth, TX. Each of these cities appear to have the closest similarity since all of their links have a small and similar height in the dendrogram. For this reason, we explore these five cities further. First we check to see how the dendrogram changes when the population demographics are removed to ensure that we have fairly similar top rated venues around the respective city centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate how the dendrogram changes without population demographics\n",
    "data_scaled_no_pop = normalize(features_df.drop(columns=['2018 Estimate', '% Change (Pop)']))\n",
    "data_scaled_no_pop = pd.DataFrame(data_scaled_no_pop, columns=features_df.drop(columns=['2018 Estimate', '% Change (Pop)']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_no_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering among most populous US cities without pop parameters (venue data as features only)\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.title('Most Populous Cities Based on Top-Rated Venue Type Only', fontsize=20)\n",
    "dend = shc.dendrogram(shc.linkage(data_scaled_no_pop, method='ward'), leaf_label_func=llf, leaf_font_size=12)\n",
    "plt.axhline(y=0.0006, color='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the linkage height of the five cities: Seattle, WA; Denver, CO; Charlotte, NC; Austin, TX; and Fort Worth, TX; we see that these cities are fairly similar in top rated venue type. Therefore, we cluster the top rated venues for each city to see which venues appear to be most common amongst them. We cluster venues within each city using k-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denver, Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Denver, Colorado'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_maps['Denver, Colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Denver, Colorado'][cities_dfs['Denver, Colorado']['postalCode'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Nominatim to identify zip code of missing venue zips\n",
    "from geopy.geocoders import Nominatim\n",
    "  \n",
    "# initialize Nominatim API \n",
    "geolocator = Nominatim(user_agent=\"denver_explorer\")\n",
    "  \n",
    "# assign venue address input and get location info\n",
    "place = cities_dfs['Denver, Colorado'].loc[61, 'address'] + ', Denver, Colorado'\n",
    "location = geolocator.geocode(place)\n",
    "  \n",
    "# traverse the data\n",
    "data = location.raw\n",
    "loc_data = data['display_name'].split()\n",
    "print(\"Full Location\")\n",
    "print(loc_data)\n",
    "print(\"Zip code : \",loc_data[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Denver, Colorado'].loc[61, 'postalCode'] = loc_data[-3].replace(',', '')\n",
    "cities_dfs['Denver, Colorado'].loc[61, 'postalCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Denver, Colorado'][cities_dfs['Denver, Colorado']['postalCode'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denver_df = cities_dfs['Denver, Colorado'][['name', 'categories', 'lat', 'lng', 'postalCode']]\n",
    "denver_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denver_grouped_zip = denver_df.groupby('postalCode').count()\n",
    "denver_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique categories.'.format(len(denver_df['categories'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "ax = denver_grouped_zip['categories'].plot(kind='barh', fontsize=12)\n",
    "ax.set_title('Distribution of Denver\\'s Top-Rated Venues by Zip Code', fontsize=20)\n",
    "ax.set_xlabel('Frequency', fontsize=15)\n",
    "ax.set_ylabel('Postal Code', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denver_grouped_zip = denver_df.groupby('postalCode').count()\n",
    "denver_grouped_zip = denver_grouped_zip.reset_index()\n",
    "denver_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denver_zip_map = folium.Map(location=[high_influx_df.loc[high_influx_df['City'] == 'Denver', 'Lat'], \n",
    "                                  high_influx_df.loc[high_influx_df['City'] == 'Denver','Long']],\n",
    "                        zoom_start=11)\n",
    "\n",
    "denver_grouped_zip['postalCode'] = denver_grouped_zip['postalCode'].astype('str')\n",
    "\n",
    "folium.Choropleth(geo_data='Colorado_ZIP_Code_Tabulation_Areas_(ZCTA).geojson',\n",
    "             data=denver_grouped_zip, # my dataset\n",
    "             columns=['postalCode', 'categories'], # zip code is here for matching the geojson zipcode, sales price is the column that changes the color of zipcode areas\n",
    "             key_on='feature.properties.ZCTA5CE10', # this path contains zipcodes in str type, this zipcodes should match with our ZIP CODE column\n",
    "             fill_color='BuPu', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='categories').add_to(denver_zip_map)\n",
    "\n",
    "denver_zip_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seattle, Washington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Seattle, Washington'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_maps['Seattle, Washington']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Seattle, Washington'][cities_dfs['Seattle, Washington']['postalCode'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df = cities_dfs['Seattle, Washington'][['name', 'categories', 'lat', 'lng', 'postalCode']]\n",
    "seattle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_grouped_zip = seattle_df.groupby('postalCode').count()\n",
    "seattle_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique categories.'.format(len(seattle_df['categories'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "ax = seattle_grouped_zip['categories'].plot(kind='barh', fontsize=12)\n",
    "ax.set_title('Distribution of Seattle\\'s Top-Rated Venues by Zip Code', fontsize=20)\n",
    "ax.set_xlabel('Frequency', fontsize=15)\n",
    "ax.set_ylabel('Postal Code', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_grouped_zip = seattle_df.groupby('postalCode').count()\n",
    "seattle_grouped_zip = seattle_grouped_zip.reset_index()\n",
    "seattle_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_zip_map = folium.Map(location=[high_influx_df.loc[high_influx_df['City'] == 'Seattle', 'Lat'], \n",
    "                                  high_influx_df.loc[high_influx_df['City'] == 'Seattle','Long']],\n",
    "                        zoom_start=12)\n",
    "\n",
    "seattle_grouped_zip['postalCode'] = seattle_grouped_zip['postalCode'].astype('str')\n",
    "\n",
    "folium.Choropleth(geo_data='wa_washington_zip_codes_geo.min.json',\n",
    "             data=seattle_grouped_zip, # my dataset\n",
    "             columns=['postalCode', 'categories'], # zip code is here for matching the geojson zipcode, sales price is the column that changes the color of zipcode areas\n",
    "             key_on='feature.properties.ZCTA5CE10', # this path contains zipcodes in str type, this zipcodes should match with our ZIP CODE column\n",
    "             fill_color='BuPu', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Popular Venues').add_to(seattle_zip_map)\n",
    "\n",
    "seattle_zip_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charlotte, North Carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Charlotte, North Carolina'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_maps['Charlotte, North Carolina']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlotte_null_zips = cities_dfs['Charlotte, North Carolina'][cities_dfs['Charlotte, North Carolina']['postalCode'].isnull()]\n",
    "charlotte_null_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Nominatim API \n",
    "geolocator = Nominatim(user_agent=\"charlotte_explorer\")\n",
    "\n",
    "# assign venue address input and get location info\n",
    "for i in charlotte_null_zips.index:\n",
    "    place = cities_dfs['Charlotte, North Carolina'].loc[i, 'address'] + ', Charlotte, North Carolina'\n",
    "    location = geolocator.geocode(place)\n",
    "    \n",
    "    # traverse the data\n",
    "    data = location.raw\n",
    "    loc_data = data['display_name'].split()\n",
    "    print(\"Full Location\")\n",
    "    print(loc_data)\n",
    "    print(\"Zip code : \",loc_data[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Charlotte, North Carolina'].loc[29, 'postalCode'] = '28204'\n",
    "cities_dfs['Charlotte, North Carolina'].loc[48, 'postalCode'] = loc_data[-3].replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlotte_df = cities_dfs['Charlotte, North Carolina'][['name', 'categories', 'lat', 'lng', 'postalCode']]\n",
    "charlotte_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlotte_grouped_zip = charlotte_df.groupby('postalCode').count()\n",
    "charlotte_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique categories.'.format(len(charlotte_df['categories'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "ax = charlotte_grouped_zip['categories'].plot(kind='barh', fontsize=12)\n",
    "ax.set_title('Distribution of Charlotte\\'s Top-Rated Venues by Zip Code', fontsize=20)\n",
    "ax.set_xlabel('Frequency', fontsize=15)\n",
    "ax.set_ylabel('Postal Code', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlotte_grouped_zip = charlotte_df.groupby('postalCode').count()\n",
    "charlotte_grouped_zip = charlotte_grouped_zip.reset_index()\n",
    "charlotte_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlotte_zip_map = folium.Map(location=[high_influx_df.loc[high_influx_df['City'] == 'Charlotte', 'Lat'], \n",
    "                                  high_influx_df.loc[high_influx_df['City'] == 'Charlotte','Long']],\n",
    "                        zoom_start=12)\n",
    "\n",
    "charlotte_grouped_zip['postalCode'] = charlotte_grouped_zip['postalCode'].astype('str')\n",
    "\n",
    "folium.Choropleth(geo_data='nc_north_carolina_zip_codes_geo.min.json',\n",
    "             data=charlotte_grouped_zip, # my dataset\n",
    "             columns=['postalCode', 'categories'], # zip code is here for matching the geojson zipcode, sales price is the column that changes the color of zipcode areas\n",
    "             key_on='feature.properties.ZCTA5CE10', # this path contains zipcodes in str type, this zipcodes should match with our ZIP CODE column\n",
    "             fill_color='BuPu', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Popular Venues').add_to(charlotte_zip_map)\n",
    "\n",
    "charlotte_zip_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Austin, Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Austin, Texas'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_maps['Austin, Texas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_null_zips = cities_dfs['Austin, Texas'][cities_dfs['Austin, Texas']['postalCode'].isnull()]\n",
    "austin_null_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Nominatim API \n",
    "geolocator = Nominatim(user_agent=\"austin_explorer\")\n",
    "\n",
    "# assign venue address input and get location info\n",
    "for i in austin_null_zips.index:\n",
    "#     place = str(cities_dfs['Austin, Texas'].loc[i, 'address']) + ', Austin, Texas'\n",
    "    location = geolocator.reverse((austin_null_zips.loc[i, 'lat'], austin_null_zips.loc[i, 'lng']))\n",
    "    \n",
    "    # traverse the data\n",
    "    data = location.raw\n",
    "    loc_data = data['display_name'].split()\n",
    "    print(\"Full Location\")\n",
    "    print(loc_data)\n",
    "    print(\"Zip code : \",loc_data[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in austin_null_zips.index:\n",
    "    cities_dfs['Austin, Texas'].loc[i, 'postalCode'] = loc_data[-3].replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_null_zips = cities_dfs['Austin, Texas'][cities_dfs['Austin, Texas']['postalCode'].isnull()]\n",
    "austin_null_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_df = cities_dfs['Austin, Texas'][['name', 'categories', 'lat', 'lng', 'postalCode']]\n",
    "austin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_grouped_zip = austin_df.groupby('postalCode').count()\n",
    "austin_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique categories.'.format(len(austin_df['categories'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "ax = austin_grouped_zip['categories'].plot(kind='barh', fontsize=12)\n",
    "ax.set_title('Distribution of Austin\\'s Top-Rated Venues by Zip Code', fontsize=20)\n",
    "ax.set_xlabel('Frequency', fontsize=15)\n",
    "ax.set_ylabel('Postal Code', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_grouped_zip = austin_df.groupby('postalCode').count()\n",
    "austin_grouped_zip = austin_grouped_zip.reset_index()\n",
    "austin_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_zip_map = folium.Map(location=[high_influx_df.loc[high_influx_df['City'] == 'Austin', 'Lat'], \n",
    "                                  high_influx_df.loc[high_influx_df['City'] == 'Austin','Long']],\n",
    "                        zoom_start=12)\n",
    "\n",
    "austin_grouped_zip['postalCode'] = austin_grouped_zip['postalCode'].astype('str')\n",
    "\n",
    "folium.Choropleth(geo_data='tx_texas_zip_codes_geo.min.json',\n",
    "             data=austin_grouped_zip, # my dataset\n",
    "             columns=['postalCode', 'categories'], # zip code is here for matching the geojson zipcode, sales price is the column that changes the color of zipcode areas\n",
    "             key_on='feature.properties.ZCTA5CE10', # this path contains zipcodes in str type, this zipcodes should match with our ZIP CODE column\n",
    "             fill_color='BuPu', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Popular Venues').add_to(austin_zip_map)\n",
    "\n",
    "austin_zip_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fort Worth, Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dfs['Fort Worth, Texas'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_maps['Fort Worth, Texas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fort_worth_null_zips = cities_dfs['Fort Worth, Texas'][cities_dfs['Fort Worth, Texas']['postalCode'].isnull()]\n",
    "fort_worth_null_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fort_worth_df = cities_dfs['Fort Worth, Texas'][['name', 'categories', 'lat', 'lng', 'postalCode']]\n",
    "fort_worth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fort_worth_grouped_zip = fort_worth_df.groupby('postalCode').count()\n",
    "fort_worth_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique categories.'.format(len(fort_worth_df['categories'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "ax = fort_worth_grouped_zip['categories'].plot(kind='barh', fontsize=12)\n",
    "ax.set_title('Distribution of Fort Worth\\'s Top-Rated Venues by Zip Code', fontsize=20)\n",
    "ax.set_xlabel('Frequency', fontsize=15)\n",
    "ax.set_ylabel('Postal Code', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fort_worth_grouped_zip = fort_worth_df.groupby('postalCode').count()\n",
    "fort_worth_grouped_zip = fort_worth_grouped_zip.reset_index()\n",
    "fort_worth_grouped_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fort_worth_zip_map = folium.Map(location=[high_influx_df.loc[high_influx_df['City'] == 'Fort Worth', 'Lat'], \n",
    "                                  high_influx_df.loc[high_influx_df['City'] == 'Fort Worth','Long']],\n",
    "                        zoom_start=12)\n",
    "\n",
    "fort_worth_grouped_zip['postalCode'] = fort_worth_grouped_zip['postalCode'].astype('str')\n",
    "\n",
    "folium.Choropleth(geo_data='tx_texas_zip_codes_geo.min.json',\n",
    "             data=fort_worth_grouped_zip, # my dataset\n",
    "             columns=['postalCode', 'categories'], # zip code is here for matching the geojson zipcode, sales price is the column that changes the color of zipcode areas\n",
    "             key_on='feature.properties.ZCTA5CE10', # this path contains zipcodes in str type, this zipcodes should match with our ZIP CODE column\n",
    "             fill_color='BuPu', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Popular Venues').add_to(fort_worth_zip_map)\n",
    "\n",
    "fort_worth_zip_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
